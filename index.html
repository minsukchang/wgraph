<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134247812-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134247812-1');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Workflow Graphs: A Computational Model of Collective Task Strategies for 3D Design Software</title>

    <link rel="stylesheet" href="style.css">

    <!-- bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">

    <!-- Google fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,300" rel="stylesheet" type="text/css">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">

    <style type="text/css">
        .normal {
            font-size: 20px;
        }

        .center {
            margin: auto;
            text-align: left;
            position: relative;
            width: 800px;
        }

        .center_vid {
            margin: 0 auto;
            display: block;
        }

        button {
            margin: auto;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
        }

        canvas {
            margin: auto;
            position: relative;
            display: block;
        }
    </style>
</head>

<div id="header">
    <h1>Workflow Graphs: <br> A Computational Model of Collective Task Strategies <br> for 3D Design Software</h1>
    <div style="clear:both;"></div>
</div>

<div class="container sec" align="center">
    <a href="http://cs.kaist.ac.kr/">
        <img src="img/kaist_logo.jpg" style="height:50px; margin-right: 20px;">
    </a>
    <a href="https://research.autodesk.com/">
        <img src="img/autodesk_research_rgb_stacked_large.png" style="height:40px;  margin-right: 20px;">
    </a>
    <a href="https://www.dgp.toronto.edu/">
        <img src="img/computer-science-university-of-toronto-logo.png" style="height:50px;  margin-right: 20px;">
    </a>
</div>


<div class="sechighlight" align="center">
            <div class="instructor" style="width:150px">
                <a href="https://www.minsukchang.com">
                    <div class="instructorphoto"><img src="img/minsuk.png"></div>
                    <div>Minsuk Chang</div>
                </a>
                <div>KAIST<br><br></div>
            </div>
            <div class="instructor" style="width:150px">
                <a href="http://www.benlafreniere.ca/">
                    <div class="instructorphoto"><img src="img/ben.png"></div>
                    <div>Ben Lafreniere</div>
                </a>
                <div>(prev. Autodesk Research) Chatham Labs</div>
            </div>                
            <div class="instructor" style="width:150px">
                <a href="http://juhokim.com">
                    <div class="instructorphoto"><img src="img/juho.jpg"></div>
                    <div style="width:150px">Juho Kim</div>
                </a>
                <div>KAIST<br><br></div>
            </div>                
            <div class="instructor" style="width:150px">
                <a href="https://www.autodeskresearch.com/people/george-fitzmaurice">
                    <div class="instructorphoto"><img src="img/george.png"></div>
                    <div>George Fitzmaurice</div>
                </a>
                <div>Autodesk Research<br><br></div>
            </div>
            <div class="instructor" style="width:150px">
                <a href="https://tovigrossman.com">
                    <div class="instructorphoto"><img src="img/tovi2.png"></div>
                    <div style="width:150px">Tovi Grossman</div>
                </a>
                <div>University of Toronto<br><br></div>
            </div> 
    </div>
</div>

<div class="container sec"> <!-- For Video if we have one... -->
    <h3 align="center">Video Preview</h2>
    <div align="center">
            <iframe width="640" height="360" align="middle" src="wgraph_gi2020.mp4"
                frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
    </div>
</div>

<div class="sechighlight">
    <div class="container sec"> 
        
        <h2>Abstract</h2>
        <div id="coursedesc" style="font-size:18px">
            This paper introduces Workflow graphs, or W-graphs, which encode how the approaches taken 
            by multiple users performing a fixed 3D design task converge and diverge from one another. 
            The graph’s nodes represent equivalent intermediate task states across users, and directed edges represent 
            how a user moved between these states, inferred from screen recording videos, command log data, and task content history. 
            The result is a data structure that captures alternative methods for performing sub-tasks (e.g., modeling the legs of a
            chair) and alternative strategies of the overall task. As a case study, we describe and exemplify a computational 
            pipeline for building W-graphs using screen recordings, command logs, and 3D model
            snapshots from an instrumented version of the Tinkercad 3D modeling application, and present graphs built for two sample tasks.
            We also illustrate how W-graphs can facilitate novel user interfaces
            with scenarios in workflow feedback, on-demand task guidance, and instructor dashboards.
        </div>

    </div>
</div>


    <div class="container sec" style="font-size:18px">
        <h2>Workflow Graphs Concept</h2>
        <div class="row">        
            <div class="col-md-7">
                <p>
                In this research, we investigate how multiple demonstrations of a fixed task can be captured and 
                represented in a workflow graph (W-graph) (Figure 1).
                The idea is to automatically discover the different means of accomplishing a goal from the interaction traces 
                of multiple users, and to encode these in a graph representation. 
                The graph thus represents diverse understanding of the task, opening up arange of possible applications.
                </p>
            </div>
            <div class="col-md-5">
                <img src="figures/ChairGraph-03.png" width="100%" />
            </div>
        
            <div class="col-md-12">
                <div>
                    <p>
                    </p>
                </div>
            </div>

        </div>
    </div>


    <div class="sechighlight">
<div class="container sec" style="font-size:18px">
    <h2>How to Build a Workflow Graph</h2>
    <div class="row">
        
        <div class="col-md-4">
            <img src="" width="100%" />
        </div>
        <div class="col-md-8"> 
            <p></p>
        
                

        </div>
        

        <div class="col-md-12">
            <p></p>

        </div>
    </div>
    <br>

    </div>
</div>

<div class="sechighlight">
    <div class="container sec" style="font-size:18px">
        <h2>Experiment 3 - how do people "want to" use a voice-enabled video interface?</h2>
        <div class="row">
            <!-- <div class="col-md-4">
                <img src="img/woz.jpg" width="100%" />
            </div>-->
            <div class="col-md-12">
                <div>
                    <p>
                        From the previous study, we learned that users’ navigation intents affect their linguistic choices for command utterances.
                        We also observed that commonly supported voice commands are limited to simple words, 
                        that it can be difficult for users to express their intents with a restrictive command space, 
                        and that it is difficult for systems to understand the intents. For example, different backward jump intents for “stop” and
                        “pause” can only be understood in context of other commands before and after the stop, 
                        specifically analyzing preceding and succeeding commands and user goals, which is impractical in application settings 
                        where users need systems to understand the user intents in real time.
                    </p>
                    <p>
                        To inform how to disambiguate voice commands and corresponding user intents for navigating how-to videos, 
                        we conducted a Wizard-of-Oz experiment to learn how users would naturally converse for video navigation 
                        in the absence of these constraints.
                    </p>
                </div>
                <h3>Findings</h3>
                <div>
                    <b>Challenge 1 - Characteristics of How-to Videos</b> </div>
                    <p>Because of the sequential nature of the video (there is the concept of an unknown future), 
                    users often make a guess to navigate forward in the video, or they have to watch less relevant or less interesting segments.
                </p>
                    <div><b>Challenge 2 - Voice Inherent Problems</b></div>
                <p>When participants used a specific time interval for jumps, 
                    it often required multiple adjustments to navigate to the target even when the participant had a good sense of where the target was. 
                    In this case, command parsing delays become an important user interface limitation.
                </p>
                
            </div>

        </div>
    </div>
</div>


<div class="container sec" style="font-size:18px">
    <div class="row">
        <div class="col-md-12">
        <h2>Summary</h2>
        <div>
           <p> Our study and interview results highlight the challenges, opportunities, and user expectations of using voice interfaces for video tutorials.
            We first summarize the user challenges of adapting to a VUI from a GUI when learning physical tasks with video tutorials. 
            We then summarize how our research methodology of designing a series of experiments in progression can be extended to 
            designing VUI for other applications and domains.
        </p>
        </div>
        <div>
            <h3><b>Transitioning from GUI to VUI</b></h3>
            <p>
            <b>Mouse vs Voice.</b> We found voice interfaces require an initial pause while issuing subsequent commands. 
            For example, when using voice input in Study 2, users issued a pause command before every rewind command.
            In contrast, when using the traditional mouse interface, users never paused the video before skipping to another point. 
            We think this is due to the time it takes for the user to speak the voice command and for the system to process it.
            Also, the target is directly specified with mouse (or touch) but with voice the target is often specified relative to the current position of the video. 
            For example, if the user does not pause the video before jumping, the original reference keeps moving, 
            and the interval they had thought of will not get them to the point they intended. 
            As a result, the larger consequence is that voice-based interactions require more steps to achieve the same objective (i.e., pause + jump) 
            than mouse-based interactions do (i.e., click).</p>
            <p>
            <b>Uncertainty from Unseen Content.</b> When trying to navigate a video tutorial using voice, users make more concrete references to the past, 
            whereas users have challenges describing later part of the video. For traditional video interfaces, 
            scrubbing and clicking around are often used a solution to quickly peeking into the future. 
            However, for voice interfaces, such a solution does not exist yet. 
            Handling this uncertainty is an important design issue which would improve the usability of voice interactions for videos.
            Recognition of Speech Input and Command Learnability. While the concept of using voice to navigate how-to videos is generally welcomed, 
            participants also reported well-known problems of voice user interfaces. Speech recognition does not always work as expected, 
            especially if users have accents or are in a noisy environment. 
            In Study 2, nine participants also reported difficulty in figuring out the available commands. 
            All participants showed frustration when the system did not respond to their command. 
            Usability of VUI suffers due to relatively poor recognition, poor learnability and discoverability of available commands, and lack of feedback.  
            </p>
        </div>
        <div>
            <h3><b>Progression of Experiment Designs</b></h3>
            In order to understand a user-centric design of voice interfaces for video tutorials, 
            we carefully designed the three studies posing users in three scenarios in progression. 
            Starting from how users use the current interface without voice interaction, to a basic adoption of voice interaction, 
            to a Wizard-of-Oz interface with “ideal” voice interactions. We were able to create a taxonomy of current interactions, 
            classify user intents in video navigation, and understand user challenges and opportunities for eliciting design recommendations.
            We believe this progression of experiment design is generalizable to understanding how to design voice interactions 
            for new applications or other domains like driving and exercising. For example, when understanding how to design 
            voice interactions while driving, the same progression of studies could be just as effective. Understanding 
            the current practices and needs of voice interactions while driving, and then using a design probe using a voice interface probe to
            understand opportunities and challenges, and then carrying out a Wizard-of-Oz study to elicit ideal interaction scenarios
            </p>
        </div>
    </div>
    </div>
</div>

<div class="sechighlight">
    <div class="container sec" style="font-size:18px">
        
        <div class="row">
            <div class="col-md-12">
                <h2>Design Recommendations</h2>    
            <div>
                   <p> Based on our findings and understanding from the three studies, 
                    we propose the following recommendations for designing voice based navigation for how-to videos.</p>
            </div>

                <div>
                <b>Support Conversational Strategies</b> 
            </div>
            <p>
                Support sequence expansions and command queues as both are strategies users often use. 
                For example, supporting users to perform a single command multiple times in a row by recognizing “again” 
                following “go back 5 seconds”, and supporting users to place multiple commands in one utterance 
                like “go to 2 minutes and 4 second mark and pause” would be useful.
            </p>
            <div>
                <b>Support Iterative Refinements of Commands</b> 
            </div>
            <p>
                Users often need multiple tries to find the intended navigation target. 
                It is because a) what users remember can be different from the part they are looking for or vice versa, 
                b) sometimes users don’t remember, and c) sometimes users remember but don’t know the exact vocabulary 
                like the names of knitting techniques and tools. Good examples are support for descriptive commands and keyword search in transcripts.
            </p>
            <div>
                <b>Support Interactions with User Context</b> 
            </div>
            <p>
                Designing voice commands for how-to videos is not about supporting a single command, 
                but understanding the higher level user intent behind the utterance is crucial. 
                We identified all seven interaction intents (pace control pause, content alignment pause, 
                video control pause, reference jump, replay jump, skip jump, and peek jump) that can be supported. 
                One possible solution in distinguishing them is to set up the command vocabulary such that each intent has its unique keyword.
            </p>
        </div>
        </div>
    </div>
</div>




<div class="container sec" style="font-size:18px">
    <div class="row">   

        <div class="col-md-5">
                    <h2>GI 2020 Paper</h2>
                    <p> DL link </p>
                    <p> Reviews </p>
                    <h2>GI 2020 Slides </h2>
                    <p></p>
        </div>
        <div class="col-md-7">
            <h2>Bibtex</h2>
            <pre style="font-size:12px;"> Available soon </pre>
        </div>

    </div>
    <hr class="style1">






<div class="sechighlight"></div>
    <div class="container sec">
        <div id="footer">
            This research was supported by <a href="https://research.adobe.com"> Adobe Research. </a>
        </div>
    </div>
</div>

</body>

</html>
